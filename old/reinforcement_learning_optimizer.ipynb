{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 5104.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexl\\AppData\\Local\\Temp\\ipykernel_21660\\3939444133.py:128: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  state = torch.FloatTensor(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25, Total Reward: 5211.10\n",
      "Episode 50, Total Reward: 5173.54\n",
      "Episode 75, Total Reward: 5079.84\n",
      "Episode 100, Total Reward: 5163.22\n",
      "Episode 125, Total Reward: 5071.87\n",
      "Episode 150, Total Reward: 5094.73\n",
      "Episode 175, Total Reward: 5224.25\n",
      "Episode 200, Total Reward: 5082.18\n",
      "Episode 225, Total Reward: 5140.54\n",
      "Episode 250, Total Reward: 5335.52\n",
      "Episode 275, Total Reward: 5204.58\n",
      "Episode 300, Total Reward: 5162.30\n",
      "Episode 325, Total Reward: 5100.10\n",
      "Episode 350, Total Reward: 5026.66\n",
      "Episode 375, Total Reward: 5048.24\n",
      "Episode 400, Total Reward: 5116.40\n",
      "Episode 425, Total Reward: 5218.49\n",
      "Episode 450, Total Reward: 5190.51\n",
      "Episode 475, Total Reward: 5124.24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# --- Environment ---\n",
    "class EnergyTradingEnv:\n",
    "    def __init__(self, T=1.0, dt=0.05):\n",
    "        self.T = T\n",
    "        self.dt = dt\n",
    "        self.steps = int(T / dt)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.Y = 50.0  # mid price\n",
    "        self.Z = 0.0   # net position\n",
    "        self.state = np.array([self.Y, self.Z, self.t * self.dt], dtype=np.float32)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, q):\n",
    "        h = 0.1\n",
    "        varphi = 0.01 * q ** 2\n",
    "        psi = 0.05 * q\n",
    "        sigma_Y = 1.0\n",
    "        sigma_D = 1.0\n",
    "\n",
    "        P = self.Y + np.sign(q) * h + varphi\n",
    "        reward = -q * P\n",
    "\n",
    "        dW_Y = np.random.randn() * np.sqrt(self.dt)\n",
    "        dW_D = np.random.randn() * np.sqrt(self.dt)\n",
    "\n",
    "        self.Y += (0.0 + psi) * self.dt + sigma_Y * dW_Y\n",
    "        self.Z += q * self.dt + sigma_D * dW_D\n",
    "\n",
    "        self.t += 1\n",
    "        done = self.t >= self.steps\n",
    "\n",
    "        next_state = np.array([self.Y, self.Z, self.t * self.dt], dtype=np.float32)\n",
    "\n",
    "        if done:\n",
    "            reward += -self.terminal_cost(self.Y, self.Z)\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def terminal_cost(self, Y, Z):\n",
    "        # optimize over xi: simple quadratic cost C(xi) = xi^2, alpha = 0.1 * I_T^2\n",
    "        def g(xi):\n",
    "            I_T = xi + Z\n",
    "            C = xi ** 2\n",
    "            imbalance_cost = I_T * (Y + np.sign(I_T) * 0.1 + 0.1 * I_T)\n",
    "            return C - imbalance_cost\n",
    "\n",
    "        xi_range = np.linspace(0, 10, 100)\n",
    "        costs = np.array([g(xi) for xi in xi_range])\n",
    "        return np.min(costs)\n",
    "\n",
    "\n",
    "# --- Actor and Critic Networks ---\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Tanh()  # Output in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x) * 5  # scale to trading rate range\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        return self.net(torch.cat([x, a], dim=-1))\n",
    "\n",
    "\n",
    "# --- DDPG Agent ---\n",
    "class DDPG:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.actor_target = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "\n",
    "    def act(self, state, noise=0.1):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action = self.actor(state).detach().numpy()[0]\n",
    "        return action + noise * np.random.randn(*action.shape)\n",
    "\n",
    "    def remember(self, *args):\n",
    "        self.memory.append(args)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = torch.FloatTensor(action)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        done = torch.FloatTensor(done).unsqueeze(1)\n",
    "\n",
    "        # Critic update\n",
    "        with torch.no_grad():\n",
    "            target_q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "            target = reward + self.gamma * (1 - done) * target_q\n",
    "\n",
    "        current_q = self.critic(state, action)\n",
    "        critic_loss = nn.MSELoss()(current_q, target)\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        # Actor update\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # Soft update\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "if __name__ == '__main__':\n",
    "    env = EnergyTradingEnv()\n",
    "    agent = DDPG(state_dim=3, action_dim=1)\n",
    "\n",
    "    episodes = 500\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            agent.remember(state, action, reward, next_state, float(done))\n",
    "            agent.update()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        if ep % 25 == 0:\n",
    "            print(f\"Episode {ep}, Total Reward: {total_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
